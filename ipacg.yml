- "Production is down!"

Rings any bell? I heard that the week I joined the project. The same week the project was launched to production. Apparently the whole team stayed up working until around 11pm or maybe later...

Of course there were kudos for everyone at the next day for showing interest.
And of course, the **fear** for the `On Call Role` started. 

This was not the only issue: 
  - Our branching strategy was not working
  - The whole workflow was not really clear to everyone 
  - It was not optimal
  - The team didn't get along

We had blockers to develop features, and we were from experiencing a DevOps culture between us. 

Ring any bells? 

This is the first article of a series into How we started to implement a DevOps culture in a project we will call: IPAC-G

Let's jump to facts and context.

---

I-PACG BEFORE
===

Project Status: Red
Prod Servers Status: Snowflake (https://martinfowler.com/bliki/SnowflakeServer.html)
Dev Servers Status: Snowflake  
Devs: 7 (1 QA)
Dev environments: 3
Metric generation: No
Amount of repositories: 11

Infrastructure Metrics: 
---

|
      Task     | Best Case Scenario  | Worst Case Scenario |
|
-------------|----------------------|---------------------|
|
   Creation of a Dev Environment | 4 hours | 5 days |
|
   Prod deployment time | 2 hours | 2 days |
|
   Prod deployment frequency | 4 features every 2 weeks | 2 features every 2 weeks|
|
   Prod Deployments Failure Rate | 1 out of 2 deployments failed | 2 out of 2 deployments failed |
|
   Weekly Production failures | 1 | 5 |
|
   (MTTR) Media time to repair | 1 day | 2 days |

---


IPAC-G AFTER
===
Project Status: Green
Prod Servers Status: Snowflake (https://martinfowler.com/bliki/SnowflakeServer.html)
Dev Servers Status: Disposable. Stable  
Devs: 7 (1 QA)
Dev environments: 3
Metric generation: No
Amount of repositories: 2

 
|
      Task     | Besct Case Scenario  | Worst Case Scenario |
|
-------------|----------------------|---------------------|
|
   Creation of a Dev Environment | 20 minutes | 40 minutes |
|
   Production deployments | 5 features every day | 1 feature every day |
|
   NonProd Deployment | 4 hours | 2 days |
|
   Production failures / Week| 1 | 5 |
|
   (MTTR) Media time to repair | 1 day | 2 days|








BEFORE. Other metrics we had:
---

  * 4 feature deployment every two weaks (Avg)
  * 3 development environments only. 1 of them for QA Only
  * 9/10 Creation of Dev environments failed. 
  * Infrastructure code fixed to work in only one landing zone.
  * ~11 repositories to handle the whole service
  * Deploy a sandbox with different versions of our product was a mess

Mindset.
---

  - **Automation**. 
    When we used to talk about automation, the response was basically "Why would we?"
    At first, it was complicated trying to make some members of the team visualize the benefits of automation
  - **Infrastructure as code**
    A lot of manual steps happened before, so we were lost if we tried to replicate our servers. 
    Everything was fixed or hardcoded and our scripts wouldn't work out of the environment we had set. 


AFTER. Other metrics we had:
---

  * 4 feature deployment every two weaks (Avg)
  * 3 development environments only. 1 of them for QA Only
  * 9/10 NonProd deployments failed. 
  * Infrastructure code fixed to work in only one landing zone.
  * 2 repositories. One for code, one for infra.
  * We can deploy sandboxes easily with different versions

Mindset.
---

  - **Automation**. 
    When we used to talk about automation, the response was basically "Why would we?"
    At first, it was complicated trying to make some members of the team visualize the benefits of automation
  - **Infrastructure as code**
    A lot of manual steps happened before, so we were lost if we tried to replicate our servers. 
    Everything was fixed or hardcoded and our scripts wouldn't work out of the environment we had set. 

IN SUMMARY
===

This shows that we improved:

**Lead time**: Reduced by 200% (Based on deployments per day)
**MTTR**: 500%



HOW? 

Here are 6 very important things to keep in mind when starting a DevOps transformation story: 

1. Focus on the team. 
   - Tips on how to improve the team. 
2. Don't repeat yourself.
3. Focus efforts. 
   - The client asked for documents. 
   - First issue: We created a document and we put a lot of effort into it.
   - When we presented the index to the client, he didn't care. We started making a diagram to replace the doc. 
   - Talk as much as you can with the client. Don't waste efforts Making up things by yourself. 
4. Reduce complexity. 
   - This one plays along with point number 3. 
   - parametrize. Now we're able to deploy to any landing zone
5. Break things fast. Fix them fast. 
6. You cant improve what you can't measure. 
   - Break steps as much as you can. 
   - We measure cloning time, build time, each deploy phase time. 
   - This helps us understand which parts make sense to try to improve


DevOps Stuff:
- *Automated GCAPI Deployment to any LZ*.
- *Lead Time. Deploy multiple on demand features per day*. (It used to take at least two weeks)
- *Unlimited development environments on demand*. (We used to have 3 and 3 fixed environments only)
- *Avg Failure rate*: 1 failure out of 15 deployments fail. (Before: 9 out of 10 deployments failed in nonprod)
- *MTTF* (Mean time to failure): 1 low priority issue in Prod per *month*. (Before: 1 - 5 Production issues per *Week*)
- *Environments spin up time*: 30-40 mins. (It used to take at least 4 hours)
